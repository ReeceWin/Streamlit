{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from preprocess import preprocess_image\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from preprocess import preprocess_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "layers = tf.keras.layers\n",
    "models = tf.keras.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "GPU devices: []\n",
      "tf.Tensor(\n",
      "[[19. 22.]\n",
      " [43. 50.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Test GPU acceleration\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "    b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configure TensorFlow to use memory growth (prevents taking all VRAM)\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU acceleration enabled: {len(gpus)} GPU(s) available\")\n",
    "        print(f\"GPU devices: {gpus}\")\n",
    "        \n",
    "        # Set visible devices to use only GPU\n",
    "        tf.config.set_visible_devices(gpus, 'GPU')\n",
    "        \n",
    "        # Verify TensorFlow sees the GPU\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"Logical GPUs: {len(logical_gpus)}\")\n",
    "        print(\"TensorFlow will use GPU for computations\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_dir, cases_file, labels_file):\n",
    "    \"\"\"\n",
    "    Load dataset connecting images to their labels via case IDs\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): Directory containing the images\n",
    "        cases_file (str): Path to CSV file mapping cases to images\n",
    "        labels_file (str): Path to CSV file with diagnostic labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (images, labels, unique_labels, confidences) as numpy arrays\n",
    "    \"\"\"\n",
    "    # Load CSVs\n",
    "    cases_df = pd.read_csv(cases_file)\n",
    "    labels_df = pd.read_csv(labels_file)\n",
    "    print(f\"Loaded {len(cases_df)} cases and {len(labels_df)} labels\")\n",
    "    \n",
    "    # Create image to case mapping\n",
    "    image_to_case = {}\n",
    "    for _, case in cases_df.iterrows():\n",
    "        case_id = case['case_id']\n",
    "        for col in ['image_1_path', 'image_2_path', 'image_3_path']:\n",
    "            if pd.notna(case[col]):\n",
    "                # Extract image filename from path\n",
    "                img_filename = os.path.basename(case[col])\n",
    "                img_id = os.path.splitext(img_filename)[0]\n",
    "                image_to_case[img_id] = case_id\n",
    "    \n",
    "    print(f\"Mapped {len(image_to_case)} images to their cases\")\n",
    "    \n",
    "    # Create case to label mapping\n",
    "    case_to_labels = {}\n",
    "    for _, row in labels_df.iterrows():\n",
    "        case_id = row['case_id']\n",
    "        labels_str = row['dermatologist_skin_condition_on_label_name']\n",
    "        conf_str = row['dermatologist_skin_condition_confidence']\n",
    "        \n",
    "        if pd.notna(labels_str) and labels_str != '[]':\n",
    "            case_to_labels[case_id] = (labels_str, conf_str)\n",
    "    \n",
    "    print(f\"Found labels for {len(case_to_labels)} cases\")\n",
    "    \n",
    "    # Process images\n",
    "    images = []\n",
    "    labels = []\n",
    "    confidences = []\n",
    "    all_labels = set()\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(data_dir) if f.endswith('.png')]\n",
    "    print(f\"Found {len(image_files)} images in directory\")\n",
    "    \n",
    "    for image_name in image_files:\n",
    "        img_id = os.path.splitext(image_name)[0]\n",
    "        \n",
    "        # Find case for this image\n",
    "        if img_id in image_to_case:\n",
    "            case_id = image_to_case[img_id]\n",
    "            \n",
    "            # Find labels for this case\n",
    "            if case_id in case_to_labels:\n",
    "                labels_str, conf_str = case_to_labels[case_id]\n",
    "                \n",
    "                try:\n",
    "                    # Parse label data\n",
    "                    label_names = ast.literal_eval(labels_str)\n",
    "                    confidence_scores = ast.literal_eval(conf_str)\n",
    "                    \n",
    "                    # Process image\n",
    "                    img = preprocess_image(img_id)\n",
    "                    \n",
    "                    if img is not None:\n",
    "                        # Add each label with its confidence\n",
    "                        for label, confidence in zip(label_names, confidence_scores):\n",
    "                            all_labels.add(label)\n",
    "                            images.append(img)\n",
    "                            labels.append(label)\n",
    "                            confidences.append(confidence)\n",
    "                        \n",
    "                        processed_count += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "    print(f\"Successfully processed {processed_count} images\")\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(\"No valid images were found. Check your data paths.\")\n",
    "    \n",
    "    # Map labels to numerical values\n",
    "    unique_labels = sorted(list(all_labels))\n",
    "    print(f\"Found {len(unique_labels)} unique labels\")\n",
    "    \n",
    "    # Create mapping\n",
    "    label_mapping = {label: i for i, label in enumerate(unique_labels)}\n",
    "    numerical_labels = [label_mapping[label] for label in labels]\n",
    "    \n",
    "    return np.array(images), np.array(numerical_labels), unique_labels, np.array(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape=(244, 244, 3), num_classes=2):\n",
    "    \"\"\"\n",
    "    Create a simple CNN model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_loss(y_true, y_pred, confidence):\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred) * confidence\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Data directories\n",
    "data_dir = 'dataset/images/'\n",
    "cases_file = 'dataset/scin_cases.csv'\n",
    "labels_file = 'dataset/scin_labels.csv'\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Load dataset\n",
    "X, y, unique_labels, confidence = load_dataset(data_dir, cases_file, labels_file)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val, confidence_train, confidence_val = train_test_split(\n",
    "    X, y, confidence, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(unique_labels)\n",
    "model = create_model(input_shape=(224, 224, 3), num_classes=num_classes)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=lambda y_true, y_pred: weighted_loss(y_true, y_pred, confidence_train),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=10, \n",
    "    validation_data=(X_val, y_val), \n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/final_model.h5')\n",
    "print(\"Model training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
